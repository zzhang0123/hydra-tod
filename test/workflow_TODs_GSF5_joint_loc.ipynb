{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import healpy as hp\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "sys.path.append('../hydra_tod/')\n",
    "\n",
    "from astropy.coordinates import EarthLocation, AltAz, SkyCoord\n",
    "from astropy.time import Time, TimeDelta\n",
    "import astropy.units as u\n",
    "from utils import Leg_poly_proj, view_samples\n",
    "from flicker_model import sim_noise, flicker_cov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from simulation import MultiTODSimulation\n",
    "# Load the simulation data from a pickle file\n",
    "with open('multi_tod_simulation_data.pkl', 'rb') as f:\n",
    "    multi_tod_sim = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all the individual variables available for backward compatibility\n",
    "t_list = multi_tod_sim.t_list\n",
    "theta_c_setting = multi_tod_sim.theta_c_setting\n",
    "phi_c_setting = multi_tod_sim.phi_c_setting\n",
    "theta_c_rising = multi_tod_sim.theta_c_rising\n",
    "phi_c_rising = multi_tod_sim.phi_c_rising\n",
    "bool_map_setting = multi_tod_sim.bool_map_setting\n",
    "bool_map_rising = multi_tod_sim.bool_map_rising\n",
    "integrated_beam_setting = multi_tod_sim.integrated_beam_setting\n",
    "integrated_beam_rising = multi_tod_sim.integrated_beam_rising\n",
    "full_bool_map = multi_tod_sim.full_bool_map\n",
    "pixel_indices = multi_tod_sim.pixel_indices\n",
    "integrated_beam = multi_tod_sim.integrated_beam\n",
    "Tsky_operator_setting = multi_tod_sim.Tsky_operator_setting\n",
    "Tsky_operator_rising = multi_tod_sim.Tsky_operator_rising\n",
    "sky_params = multi_tod_sim.sky_params\n",
    "ntime = multi_tod_sim.ntime\n",
    "ndiode_proj = multi_tod_sim.ndiode_proj\n",
    "nd_rec_operator = multi_tod_sim.nd_rec_operator\n",
    "nd_rec_params = multi_tod_sim.nd_rec_params\n",
    "gain_proj = multi_tod_sim.gain_proj\n",
    "gain_params_setting = multi_tod_sim.gain_params_setting\n",
    "gain_params_rising = multi_tod_sim.gain_params_rising\n",
    "gains_setting = multi_tod_sim.gains_setting\n",
    "gains_rising = multi_tod_sim.gains_rising\n",
    "fc = multi_tod_sim.fc\n",
    "logfc = multi_tod_sim.logfc\n",
    "f0 = multi_tod_sim.f0\n",
    "logf0 = multi_tod_sim.logf0\n",
    "noise_setting = multi_tod_sim.noise_setting\n",
    "noise_rising = multi_tod_sim.noise_rising\n",
    "Tsys_setting = multi_tod_sim.Tsys_setting\n",
    "Tsys_rising = multi_tod_sim.Tsys_rising\n",
    "TOD_setting = multi_tod_sim.TOD_setting\n",
    "TOD_rising = multi_tod_sim.TOD_rising\n",
    "pixels_c_setting = multi_tod_sim.pixels_c_setting\n",
    "bool_map_c_setting = multi_tod_sim.bool_map_c_setting\n",
    "calibration_1_index = multi_tod_sim.calibration_1_index\n",
    "calibration_5_indices = multi_tod_sim.calibration_5_indices\n",
    "\n",
    "# Additional constants from the original code\n",
    "T_ndiode = multi_tod_sim.T_ndiode\n",
    "rec_params = multi_tod_sim.rec_params\n",
    "dtime = multi_tod_sim.dtime\n",
    "alpha = multi_tod_sim.alpha\n",
    "logf0_list = [multi_tod_sim.logf0]\n",
    "sigma_2 = multi_tod_sim.sigma_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_gain_params(proj, gains):\n",
    "    log_gains = np.log(gains)\n",
    "    log_coeffs = np.linalg.lstsq(proj, log_gains, rcond=None)[0]\n",
    "    return log_coeffs\n",
    "\n",
    "def log_local_params(nd_rec_proj, local_params):\n",
    "    log_Tnd = np.log(local_params[0])\n",
    "    loc_res = local_params[1:]\n",
    "    proj_res = nd_rec_proj[:, 1:]\n",
    "    log_locals = np.log(proj_res @ loc_res)\n",
    "    log_coeffs = np.linalg.lstsq(proj_res, log_locals, rcond=None)[0]\n",
    "    return np.concatenate(([log_Tnd], log_coeffs))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_gain_params_s = log_gain_params(gain_proj, gains_setting)\n",
    "log_gain_params_r = log_gain_params(gain_proj, gains_rising)\n",
    "\n",
    "gain_prior_mean_s = log_gain_params_s\n",
    "gain_prior_mean_r = log_gain_params_r\n",
    "gain_prior_mean_list = [gain_prior_mean_s, gain_prior_mean_r]\n",
    "gain_prior_cov_inv_s = 1. / (0.1 * gain_prior_mean_s)**2\n",
    "gain_prior_cov_inv_s[0] = 10.0\n",
    "gain_prior_cov_inv_r = 1. / (0.1 * gain_prior_mean_r)**2\n",
    "gain_prior_cov_inv_r[0] = 10.0\n",
    "gain_prior_cov_inv_list = [gain_prior_cov_inv_s, gain_prior_cov_inv_r]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_std_frac = 0.1\n",
    "Tsky_prior_cov_inv = 1. / (prior_std_frac * sky_params)**2\n",
    "calibration_indices = calibration_5_indices\n",
    "Tsky_prior_cov_inv[calibration_indices] = 1e20\n",
    "\n",
    "log_local_params_s = log_local_params(nd_rec_operator, nd_rec_params)\n",
    "log_local_params_r = log_local_params_s\n",
    "Tloc_prior_mean_list = [log_local_params_s, log_local_params_r]\n",
    "# aux = np.ones_like(nd_rec_params)*0.0\n",
    "aux = 1 / (0.1 * log_local_params_s)**2\n",
    "# aux[1] = 1 / (0.1 * log_local_params_s[1])**2\n",
    "Tloc_prior_cov_inv_list = [aux, aux]\n",
    "\n",
    "def log_prior_noise(params):\n",
    "    _, alpha_est = params\n",
    "    return -1e10 * (alpha_est - alpha)**4 \n",
    "\n",
    "init_Tloc_params_list = [log_local_params_s, log_local_params_r]\n",
    "init_Tsky_params = sky_params\n",
    "# init_Tsys_params = np.hstack([sky_params, log_local_params_s, log_local_params_r])\n",
    "init_noise_params_list = [[-4.8, 2.2], [-4.8, 2.2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the emulator for flicker noise correlation function.\n",
      "Get the JAX version of the emulators\n",
      "Get the JAX version of the log-det emulator\n",
      "Get the JAX version of the emulators\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2000 [00:00<?, ?it/s]/Users/zzhang/Workspace/flicker/notebooks/../hydra_tod/nuts_sampler.py:198: UserWarning: There are not enough devices to run parallel chains: expected 4 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(4)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.\n",
      "  mcmc = MCMC(nuts_kernel, num_warmup=current_warmup, num_samples=N_samples, num_chains=N_chains, progress_bar=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running warmup round: 1500 additional steps (total warmup: 1500)\n"
     ]
    }
   ],
   "source": [
    "# Import the FlickerCorrEmulator class first\n",
    "from full_Gibbs_sampler import TOD_Gibbs_sampler_joint_loc\n",
    "\n",
    "Tsky_samples, all_gain_samples, all_noise_samples, all_Tloc_samples = \\\n",
    "    TOD_Gibbs_sampler_joint_loc(\n",
    "        [TOD_setting, TOD_rising],\n",
    "        [t_list, t_list],\n",
    "        [gain_proj, gain_proj],\n",
    "        [Tsky_operator_setting, Tsky_operator_rising],\n",
    "        [nd_rec_operator, nd_rec_operator],\n",
    "        init_Tsky_params,\n",
    "        init_Tloc_params_list,\n",
    "        init_noise_params_list,\n",
    "        [logfc, logfc],\n",
    "        wnoise_var=2.5e-6,\n",
    "        Tsky_prior_cov_inv=Tsky_prior_cov_inv,\n",
    "        Tsky_prior_mean=sky_params,\n",
    "        local_Tloc_prior_cov_inv_list=Tloc_prior_cov_inv_list,\n",
    "        local_Tloc_prior_mean_list=Tloc_prior_mean_list,\n",
    "        local_gain_prior_cov_inv_list=gain_prior_cov_inv_list,\n",
    "        local_gain_prior_mean_list=gain_prior_mean_list,\n",
    "        local_noise_prior_func_list=[log_prior_noise, log_prior_noise],\n",
    "        noise_sampler_type=\"emcee\",\n",
    "        ploc_Jeffreys_prior=True, \n",
    "        noise_Jeffreys_prior=True,\n",
    "        n_samples=2000,\n",
    "        tol=1e-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, save the samples as npy files..\n",
    "\n",
    "np.save(\"outputs/GSF5_db/Tsky_samples_joint_loc.npy\", Tsky_samples)\n",
    "np.save(\"outputs/GSF5_db/gain_samples_joint_loc.npy\", np.concatenate(all_gain_samples, axis=0))\n",
    "np.save(\"outputs/GSF5_db/noise_samples_joint_loc.npy\", np.concatenate(all_noise_samples, axis=0))\n",
    "np.save(\"outputs/GSF5_db/Tloc_samples_joint_loc.npy\", np.concatenate(all_Tloc_samples, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tsky_samples = np.load(\"outputs/GSF5_db/Tsky_samples_joint_loc.npy\")\n",
    "gain_samples = np.load(\"outputs/GSF5_db/gain_samples_joint_loc.npy\")\n",
    "noise_samples = np.load(\"outputs/GSF5_db/noise_samples_joint_loc.npy\")\n",
    "Tloc_samples = np.load(\"outputs/GSF5_db/Tloc_samples_joint_loc.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MCMC_diagnostics import diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tloc_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = gain_samples[1].reshape(1, 2000, 4)\n",
    "diagnostics(aux[:, :, :], param_names=[r\"$p_{g,0}$\", r\"$p_{g,1}$\",r\"$p_{g,2}$\",r\"$p_{g,3}$\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Sample shapes:\")\n",
    "print(f\"Gain samples: {gain_samples.shape}\")\n",
    "print(f\"Noise samples: {noise_samples.shape}\")\n",
    "print(f\"Tloc samples: {Tloc_samples.shape}\")\n",
    "\n",
    "def analyze_parameter_correlations(gain_samples, noise_samples, Tloc_samples, \n",
    "                                 figsize=(16, 12), save_path=None):\n",
    "    \"\"\"\n",
    "    Comprehensive correlation analysis between gain, noise, and Tloc parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reshape samples if needed\n",
    "    n_chains_gain = gain_samples.shape[0] if len(gain_samples.shape) > 2 else 1\n",
    "    n_chains_noise = noise_samples.shape[0] if len(noise_samples.shape) > 2 else 1\n",
    "    n_chains_tloc = Tloc_samples.shape[0] if len(Tloc_samples.shape) > 2 else 1\n",
    "    \n",
    "    # Flatten chains if multiple chains exist\n",
    "    if len(gain_samples.shape) == 3:\n",
    "        gain_flat = gain_samples.reshape(-1, gain_samples.shape[-1])\n",
    "    else:\n",
    "        gain_flat = gain_samples\n",
    "        \n",
    "    if len(noise_samples.shape) == 3:\n",
    "        noise_flat = noise_samples.reshape(-1, noise_samples.shape[-1])\n",
    "    else:\n",
    "        noise_flat = noise_samples\n",
    "        \n",
    "    if len(Tloc_samples.shape) == 3:\n",
    "        tloc_flat = Tloc_samples.reshape(-1, Tloc_samples.shape[-1])\n",
    "    else:\n",
    "        tloc_flat = Tloc_samples\n",
    "    \n",
    "    # Create parameter names\n",
    "    n_gain_params = gain_flat.shape[1] if len(gain_flat.shape) > 1 else 1\n",
    "    n_noise_params = noise_flat.shape[1] if len(noise_flat.shape) > 1 else 1 \n",
    "    n_tloc_params = tloc_flat.shape[1] if len(tloc_flat.shape) > 1 else 1\n",
    "    \n",
    "    param_names = []\n",
    "    param_names.extend([f'Gain_{i}' for i in range(n_gain_params)])\n",
    "    param_names.extend([f'Noise_{i}' for i in range(n_noise_params)])\n",
    "    param_names.extend([f'Tloc_{i}' for i in range(n_tloc_params)])\n",
    "    \n",
    "    # Combine all parameters\n",
    "    all_samples = np.column_stack([gain_flat, noise_flat, tloc_flat])\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = np.corrcoef(all_samples.T)\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    gs = fig.add_gridspec(2, 2, height_ratios=[2, 1], width_ratios=[2, 1])\n",
    "    \n",
    "    # Main correlation heatmap\n",
    "    ax_main = fig.add_subplot(gs[0, 0])\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    "    \n",
    "    sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.3f', \n",
    "                xticklabels=param_names, yticklabels=param_names,\n",
    "                cmap='RdBu_r', center=0, square=True, ax=ax_main,\n",
    "                cbar_kws={'shrink': 0.8, 'label': 'Correlation Coefficient'})\n",
    "    ax_main.set_title('Parameter Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Correlation histogram\n",
    "    ax_hist = fig.add_subplot(gs[0, 1])\n",
    "    lower_triangle = corr_matrix[np.tril_indices_from(corr_matrix, k=-1)]\n",
    "    ax_hist.hist(lower_triangle, bins=30, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "    ax_hist.axvline(0, color='red', linestyle='--', alpha=0.8)\n",
    "    ax_hist.set_xlabel('Correlation Coefficient')\n",
    "    ax_hist.set_ylabel('Frequency')\n",
    "    ax_hist.set_title('Distribution of\\nCorrelation Coefficients')\n",
    "    ax_hist.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Strong correlations table\n",
    "    ax_table = fig.add_subplot(gs[1, :])\n",
    "    ax_table.axis('off')\n",
    "    \n",
    "    # Find strongest correlations (excluding diagonal)\n",
    "    strong_corrs = []\n",
    "    for i in range(len(param_names)):\n",
    "        for j in range(i+1, len(param_names)):\n",
    "            corr_val = corr_matrix[i, j]\n",
    "            if abs(corr_val) > 0.1:  # Threshold for \"strong\" correlation\n",
    "                strong_corrs.append((param_names[i], param_names[j], corr_val))\n",
    "    \n",
    "    # Sort by absolute correlation strength\n",
    "    strong_corrs.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "    \n",
    "    # Create table\n",
    "    if strong_corrs:\n",
    "        table_data = []\n",
    "        for param1, param2, corr in strong_corrs[:10]:  # Top 10\n",
    "            table_data.append([param1, param2, f'{corr:.4f}'])\n",
    "        \n",
    "        table = ax_table.table(cellText=table_data,\n",
    "                              colLabels=['Parameter 1', 'Parameter 2', 'Correlation'],\n",
    "                              cellLoc='center',\n",
    "                              loc='center',\n",
    "                              bbox=[0.1, 0.3, 0.8, 0.6])\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(10)\n",
    "        table.scale(1.2, 1.5)\n",
    "        \n",
    "        ax_table.set_title('Strongest Parameter Correlations (|r| > 0.1)', \n",
    "                          fontsize=12, fontweight='bold', y=0.95)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Correlation analysis saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return corr_matrix, param_names, strong_corrs\n",
    "\n",
    "# Run the analysis\n",
    "corr_matrix, param_names, strong_corrs = analyze_parameter_correlations(\n",
    "    gain_samples, noise_samples, Tloc_samples,\n",
    "    figsize=(18, 12),\n",
    "    save_path=\"figures/parameter_correlations_GSF5_db.pdf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pairwise_correlations(gain_samples, noise_samples, Tloc_samples, \n",
    "                              top_n=6, figsize=(15, 10), save_path=None):\n",
    "    \"\"\"\n",
    "    Create pairwise scatter plots for the most correlated parameters\n",
    "    \"\"\"\n",
    "    # Flatten samples\n",
    "    if len(gain_samples.shape) == 3:\n",
    "        gain_flat = gain_samples.reshape(-1, gain_samples.shape[-1])\n",
    "    else:\n",
    "        gain_flat = gain_samples\n",
    "        \n",
    "    if len(noise_samples.shape) == 3:\n",
    "        noise_flat = noise_samples.reshape(-1, noise_samples.shape[-1])\n",
    "    else:\n",
    "        noise_flat = noise_samples\n",
    "        \n",
    "    if len(Tloc_samples.shape) == 3:\n",
    "        tloc_flat = Tloc_samples.reshape(-1, Tloc_samples.shape[-1])\n",
    "    else:\n",
    "        tloc_flat = Tloc_samples\n",
    "    \n",
    "    # Combine all parameters\n",
    "    all_samples = np.column_stack([gain_flat, noise_flat, tloc_flat])\n",
    "    \n",
    "    # Parameter names with more descriptive labels\n",
    "    param_labels = []\n",
    "    param_labels.extend([f'Gain p_{{{i}}}' for i in range(gain_flat.shape[1])])\n",
    "    param_labels.extend([f'log f_0, α' if i < 2 else f'Noise_{{{i}}}' for i in range(noise_flat.shape[1])])\n",
    "    param_labels.extend([f'T_{{loc,{i}}}' for i in range(tloc_flat.shape[1])])\n",
    "    \n",
    "    # Calculate correlations and find strongest ones\n",
    "    n_params = all_samples.shape[1]\n",
    "    correlations = []\n",
    "    \n",
    "    for i in range(n_params):\n",
    "        for j in range(i+1, n_params):\n",
    "            corr, p_val = pearsonr(all_samples[:, i], all_samples[:, j])\n",
    "            correlations.append((i, j, corr, p_val, param_labels[i], param_labels[j]))\n",
    "    \n",
    "    # Sort by absolute correlation\n",
    "    correlations.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "    \n",
    "    # Create subplot grid\n",
    "    rows = (top_n + 2) // 3\n",
    "    cols = min(3, top_n)\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "    if rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for idx, (i, j, corr, p_val, label_i, label_j) in enumerate(correlations[:top_n]):\n",
    "        row = idx // cols\n",
    "        col = idx % cols\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        # Create hexbin plot\n",
    "        hb = ax.hexbin(all_samples[:, i], all_samples[:, j], \n",
    "                      gridsize=30, cmap='Blues', alpha=0.8)\n",
    "        \n",
    "        # Add trend line\n",
    "        z = np.polyfit(all_samples[:, i], all_samples[:, j], 1)\n",
    "        p = np.poly1d(z)\n",
    "        x_trend = np.linspace(all_samples[:, i].min(), all_samples[:, i].max(), 100)\n",
    "        ax.plot(x_trend, p(x_trend), \"red\", alpha=0.8, linewidth=2)\n",
    "        \n",
    "        # Labels and title\n",
    "        ax.set_xlabel(label_i, fontsize=11)\n",
    "        ax.set_ylabel(label_j, fontsize=11)\n",
    "        ax.set_title(f'r = {corr:.3f}' + (f', p < 0.001' if p_val < 0.001 else f', p = {p_val:.3f}'), \n",
    "                    fontsize=12, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Remove empty subplots\n",
    "    for idx in range(top_n, rows * cols):\n",
    "        row = idx // cols\n",
    "        col = idx % cols\n",
    "        fig.delaxes(axes[row, col])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Pairwise correlations saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return correlations[:top_n]\n",
    "\n",
    "# Create pairwise plots\n",
    "top_correlations = plot_pairwise_correlations(\n",
    "    gain_samples, noise_samples, Tloc_samples,\n",
    "    top_n=9,  # 3x3 grid\n",
    "    figsize=(16, 12),\n",
    "    save_path=\"figures/pairwise_correlations_GSF5_db.pdf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_evolution(gain_samples, noise_samples, Tloc_samples, \n",
    "                             window_size=200, figsize=(14, 8), save_path=None):\n",
    "    \"\"\"\n",
    "    Plot how correlations evolve during the sampling process\n",
    "    \"\"\"\n",
    "    # Flatten samples (assuming shape is [n_chains, n_samples, n_params])\n",
    "    if len(gain_samples.shape) == 3:\n",
    "        gain_flat = gain_samples[0]  # Use first chain\n",
    "    else:\n",
    "        gain_flat = gain_samples\n",
    "        \n",
    "    if len(noise_samples.shape) == 3:\n",
    "        noise_flat = noise_samples[0]\n",
    "    else:\n",
    "        noise_flat = noise_samples\n",
    "        \n",
    "    if len(Tloc_samples.shape) == 3:\n",
    "        tloc_flat = Tloc_samples[0]\n",
    "    else:\n",
    "        tloc_flat = Tloc_samples\n",
    "    \n",
    "    all_samples = np.column_stack([gain_flat, noise_flat, tloc_flat])\n",
    "    n_samples, n_params = all_samples.shape\n",
    "    \n",
    "    # Calculate rolling correlations\n",
    "    sample_points = range(window_size, n_samples, window_size//4)\n",
    "    \n",
    "    # Select a few interesting parameter pairs\n",
    "    interesting_pairs = [\n",
    "        (0, n_params-2),  # First gain param vs first noise param\n",
    "        (0, n_params-1),  # First gain param vs second noise param  \n",
    "        (1, n_params-2),  # Second gain param vs first noise param\n",
    "        (0, gain_flat.shape[1]),  # First gain param vs first Tloc param\n",
    "    ]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    param_labels = []\n",
    "    param_labels.extend([f'Gain_{i}' for i in range(gain_flat.shape[1])])\n",
    "    param_labels.extend([f'Noise_{i}' for i in range(noise_flat.shape[1])])\n",
    "    param_labels.extend([f'Tloc_{i}' for i in range(tloc_flat.shape[1])])\n",
    "    \n",
    "    for idx, (i, j) in enumerate(interesting_pairs):\n",
    "        if idx >= 4:\n",
    "            break\n",
    "            \n",
    "        correlations = []\n",
    "        for end_point in sample_points:\n",
    "            start_point = max(0, end_point - window_size)\n",
    "            window_data_i = all_samples[start_point:end_point, i]\n",
    "            window_data_j = all_samples[start_point:end_point, j]\n",
    "            corr, _ = pearsonr(window_data_i, window_data_j)\n",
    "            correlations.append(corr)\n",
    "        \n",
    "        ax = axes[idx]\n",
    "        ax.plot(sample_points, correlations, 'b-', linewidth=2, alpha=0.8)\n",
    "        ax.axhline(0, color='red', linestyle='--', alpha=0.5)\n",
    "        ax.set_xlabel('Sample Number')\n",
    "        ax.set_ylabel('Correlation')\n",
    "        ax.set_title(f'{param_labels[i]} vs {param_labels[j]}')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Correlation evolution saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Plot correlation evolution\n",
    "plot_correlation_evolution(\n",
    "    gain_samples, noise_samples, Tloc_samples,\n",
    "    window_size=300,\n",
    "    save_path=\"figures/correlation_evolution_GSF5_db.pdf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_correlation_summary(gain_samples, noise_samples, Tloc_samples):\n",
    "    \"\"\"\n",
    "    Print summary statistics for parameter correlations\n",
    "    \"\"\"\n",
    "    # Flatten samples\n",
    "    if len(gain_samples.shape) == 3:\n",
    "        gain_flat = gain_samples.reshape(-1, gain_samples.shape[-1])\n",
    "        noise_flat = noise_samples.reshape(-1, noise_samples.shape[-1])\n",
    "        tloc_flat = Tloc_samples.reshape(-1, Tloc_samples.shape[-1])\n",
    "    else:\n",
    "        gain_flat = gain_samples\n",
    "        noise_flat = noise_samples  \n",
    "        tloc_flat = Tloc_samples\n",
    "    \n",
    "    print(\"=== PARAMETER CORRELATION SUMMARY ===\\n\")\n",
    "    \n",
    "    # Within-group correlations\n",
    "    print(\"GAIN PARAMETER CORRELATIONS:\")\n",
    "    gain_corr = np.corrcoef(gain_flat.T)\n",
    "    for i in range(gain_flat.shape[1]):\n",
    "        for j in range(i+1, gain_flat.shape[1]):\n",
    "            corr = gain_corr[i, j]\n",
    "            print(f\"  Gain_{i} - Gain_{j}: {corr:.4f}\")\n",
    "    \n",
    "    print(\"\\nNOISE PARAMETER CORRELATIONS:\")\n",
    "    noise_corr = np.corrcoef(noise_flat.T)\n",
    "    for i in range(noise_flat.shape[1]):\n",
    "        for j in range(i+1, noise_flat.shape[1]):\n",
    "            corr = noise_corr[i, j]\n",
    "            print(f\"  Noise_{i} - Noise_{j}: {corr:.4f}\")\n",
    "    \n",
    "    print(\"\\nTLOC PARAMETER CORRELATIONS:\")\n",
    "    tloc_corr = np.corrcoef(tloc_flat.T)\n",
    "    for i in range(tloc_flat.shape[1]):\n",
    "        for j in range(i+1, tloc_flat.shape[1]):\n",
    "            corr = tloc_corr[i, j]\n",
    "            print(f\"  Tloc_{i} - Tloc_{j}: {corr:.4f}\")\n",
    "    \n",
    "    # Cross-group correlations\n",
    "    print(\"\\nCROSS-GROUP CORRELATIONS (|r| > 0.05):\")\n",
    "    all_samples = np.column_stack([gain_flat, noise_flat, tloc_flat])\n",
    "    full_corr = np.corrcoef(all_samples.T)\n",
    "    \n",
    "    param_types = ['Gain']*gain_flat.shape[1] + ['Noise']*noise_flat.shape[1] + ['Tloc']*tloc_flat.shape[1]\n",
    "    param_indices = list(range(gain_flat.shape[1])) + list(range(noise_flat.shape[1])) + list(range(tloc_flat.shape[1]))\n",
    "    \n",
    "    for i in range(len(param_types)):\n",
    "        for j in range(i+1, len(param_types)):\n",
    "            if param_types[i] != param_types[j]:  # Cross-group only\n",
    "                corr = full_corr[i, j]\n",
    "                if abs(corr) > 0.05:\n",
    "                    print(f\"  {param_types[i]}_{param_indices[i]} - {param_types[j]}_{param_indices[j]}: {corr:.4f}\")\n",
    "\n",
    "# Print summary\n",
    "print_correlation_summary(gain_samples, noise_samples, Tloc_samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TOD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
